{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nb_preprocess.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWt+i0pdSPlemiSGKg7XQM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elwing-Chou/ml0602/blob/main/nb_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoAKC5bAWqaz"
      },
      "source": [
        "!pip install opencc-python-reimplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyXNp3l2X4TW"
      },
      "source": [
        "to_convert = \"\"\"训练词向量等一些任务的时候，往往需要一些较大规模的中文语料，而维基百科语料是一个很好的选择。维基百科每段时间都会备份数据，我们可以选择不同时间段的语料来下载使用。​\n",
        "\n",
        "1.下载维基百科语料\n",
        "下面就是维基百科的语料的下载地址：\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "HZRWRz-kXQcq",
        "outputId": "cd070a5f-de89-4cc9-b818-c3aa36dc5992"
      },
      "source": [
        "from opencc import OpenCC\n",
        "cc = OpenCC('s2tw')\n",
        "converted = cc.convert(to_convert)\n",
        "converted"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'訓練詞向量等一些任務的時候，往往需要一些較大規模的中文語料，而維基百科語料是一個很好的選擇。維基百科每段時間都會備份數據，我們可以選擇不同時間段的語料來下載使用。\\u200b\\n\\n1.下載維基百科語料\\n下面就是維基百科的語料的下載地址：\\n\\n維基百科的下載地址\\n\\u200b\\ndumps.wikimedia.org\\n\\n可以根據自己的需求選擇不同的語料\\n選擇其中某一個時間段會發現其中有很多下載地址鏈接，可以根據自己的需求下載不同的語料內容。\\n\\n\\n不同鏈接地址對應不同內容\\n由於我們的需求是訓練詞向量，所以我選擇\"zhwiki-20180320-pages-articles1.xml-p1p231819.bz2\"也就是包含詞條正文的鏈接進行下載（為了演示，我選擇189M包進行下載，當然下載的包越大，使用其訓練的詞向量也就越精確），下載完成後可以看到是一個壓縮包的文件，先不著急去解壓縮它，因為後面我們會直接使用工具對壓縮包進行處理。'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUmBw5IIdm9j"
      },
      "source": [
        "with open(\"./news.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brzt9lEegeSK",
        "outputId": "9f49fe17-27fe-4101-c7f2-6431fc349209"
      },
      "source": [
        "import jieba.analyse\n",
        "# topK:None (全部列出, 高到低)\n",
        "print(jieba.analyse.extract_tags(content))\n",
        "print(jieba.analyse.extract_tags(content, topK=5, allowPOS=[\"ns\", \"nr\"]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['蘋果', '美國', '兩名', '和解', '大生', 'iPhone', '數百萬', '維修', '女學生', '隱私', '訴訟', '影片', '私密', '照片', '沙加', '此事', '送修', '支付', '拒付', '俄勒岡州']\n",
            "['美國', '和碩', '沙加', '俄勒岡州', '英國']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}